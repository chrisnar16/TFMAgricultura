{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b5ba023-108d-4b74-8331-e78a7054004f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semillas aleatorias configuradas a: 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f95b9f47b30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from CherryTreeDataset import CherryTreeDataset\n",
    "from torchvision import transforms\n",
    "from funciones_auxiliares import plot_spectra, analyze_image, analyze_tiff_metadata, PATH, crop_central_region, set_seed, seed_worker\n",
    "from resnet_adapters import adapt_resnet_channels\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler, Subset\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet18_Weights, ResNet50_Weights\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score\n",
    "\n",
    "set_seed(42)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "142e2225-0e32-410c-a7d2-2e1b1ebb5ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El dispositivo seleccionado es cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'El dispositivo seleccionado es {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbb8529a-eba4-47c5-82bc-921a0b5924ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define las transformaciones si son necesarias\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((1280,960)),\n",
    "    #transforms.Lambda(lambda x: crop_central_region(x, center_ratio=0.8)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "#formats = ('RGB.JPG', 'RED.TIF','GRE.TIF','NIR.TIF','REG.TIF')\n",
    "#formats = ('RGB.JPG','NIR.TIF','REG.TIF')\n",
    "formats = ('RGB.JPG',)\n",
    "input_channels = 3\n",
    "dataset = CherryTreeDataset(PATH, transform=transform, formats = formats, concatenate = True, balance=False)\n",
    "\n",
    "healthy_indices = []\n",
    "disease_indices = []\n",
    "    \n",
    "for i, (_, label) in enumerate(dataset.samples):\n",
    "    if label == 0:  # Healthy\n",
    "        healthy_indices.append(i)\n",
    "    else:  # Disease\n",
    "        disease_indices.append(i)\n",
    "    \n",
    "    # Dividir los índices de árboles sanos en entrenamiento y validación\n",
    "np.random.shuffle(healthy_indices)\n",
    "\n",
    "train_healthy = healthy_indices[:int(0.8 * len(healthy_indices))]\n",
    "test_healthy = healthy_indices[int(0.8 * len(healthy_indices)):]\n",
    "train_disease = disease_indices[:int(0.8 * len(disease_indices))]\n",
    "test_disease = disease_indices[int(0.8 * len(disease_indices)):]\n",
    "    \n",
    "    # Los índices de árboles enfermos solo se usan para testing\n",
    "    # Create datasets\n",
    "train_dataset = Subset(dataset, train_healthy)\n",
    "val_dataset = Subset(dataset, test_healthy + test_disease)  # Incluimos enfermos solo para evaluación\n",
    "    \n",
    "    # Crear dataloader\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4, worker_init_fn=seed_worker)\n",
    "test_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4, worker_init_fn=seed_worker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd6dc02b-e4eb-46cc-974a-32f5ac7526b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, input_channels, latent_dim=128):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder: Reducción progresiva de la dimensionalidad\n",
    "        self.encoder = nn.Sequential(\n",
    "            # Primera capa convolucional (1280x960 -> 640x480)\n",
    "            nn.Conv2d(input_channels, 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # Segunda capa convolucional (640x480 -> 320x240)\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # Tercera capa convolucional (320x240 -> 160x120)\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # Cuarta capa convolucional (160x120 -> 80x60)\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # Quinta capa convolucional (80x60 -> 40x30)\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # Sexta capa convolucional (40x30 -> 20x15)\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        \n",
    "        # Tamaño del feature map después del encoder\n",
    "        self.feature_size = (20, 15)\n",
    "        self.encoder_output_dim = 512 * self.feature_size[0] * self.feature_size[1]\n",
    "        \n",
    "        # Capas para calcular mu y logvar (parámetros de la distribución latente)\n",
    "        self.fc_mu = nn.Linear(self.encoder_output_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(self.encoder_output_dim, latent_dim)\n",
    "        \n",
    "        # Capa para transformar el espacio latente de vuelta a la forma adecuada para el decoder\n",
    "        self.fc_decoder = nn.Linear(latent_dim, 512 * self.feature_size[0] * self.feature_size[1])\n",
    "        \n",
    "        # Decodificador\n",
    "        self.decoder = nn.Sequential(\n",
    "            # Primera capa de transposición convolucional (20x15 -> 40x30)\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # Segunda capa de transposición convolucional (40x30 -> 80x60)\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # Tercera capa de transposición convolucional (80x60 -> 160x120)\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # Cuarta capa de transposición convolucional (160x120 -> 320x240)\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # Quinta capa de transposición convolucional (320x240 -> 640x480)\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # Sexta capa de transposición convolucional (640x480 -> 1280x960)\n",
    "            nn.ConvTranspose2d(16, input_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()  # Para normalizar la salida entre 0 y 1\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        # Pasa la entrada por el encoder\n",
    "        x = self.encoder(x)\n",
    "        # Aplanar para las capas fully connected\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Devuelve mu y logvar\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        # Técnica de reparametrización con mayor estabilidad numérica\n",
    "        # Aseguramos que logvar esté en un rango razonable\n",
    "        logvar = torch.clamp(logvar, min=-20, max=20)\n",
    "        \n",
    "        # Calculamos la desviación estándar\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        \n",
    "        # Limitamos también la desviación estándar para evitar valores extremos\n",
    "        std = torch.clamp(std, min=1e-6, max=10)\n",
    "        \n",
    "        # Generamos ruido aleatorio\n",
    "        eps = torch.randn_like(std)\n",
    "        \n",
    "        # Técnica de reparametrización\n",
    "        z = mu + eps * std\n",
    "        return z\n",
    "    \n",
    "    def decode(self, z):\n",
    "        # Transforma desde espacio latente a forma adecuada para el decoder\n",
    "        z = self.fc_decoder(z)\n",
    "        z = z.view(z.size(0), 512, self.feature_size[0], self.feature_size[1])\n",
    "        # Pasa por el decoder\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encode -> reparameterize -> decode\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        reconstructed = self.decode(z)\n",
    "        return reconstructed, mu, logvar\n",
    "\n",
    "class VAEAnomalyDetector:\n",
    "    def __init__(self, input_channels, device='cuda', latent_dim=128):\n",
    "        self.device = device\n",
    "        self.model = VariationalAutoencoder(input_channels, latent_dim).to(device)\n",
    "        self.input_channels = input_channels\n",
    "        self.threshold = None\n",
    "        self.latent_dim = latent_dim\n",
    "    \n",
    "    def vae_loss(self, recon_x, x, mu, logvar, kld_weight=0.0001):\n",
    "        \"\"\"\n",
    "        Función de pérdida para VAE: Reconstrucción + KL Divergence\n",
    "        Con mejor estabilidad numérica\n",
    "        \"\"\"\n",
    "        # Error de reconstrucción (MSE) - usando reducción 'mean' para estabilidad\n",
    "        recon_loss = F.mse_loss(recon_x, x, reduction='mean') * x.size(0) * x.size(1) * x.size(2) * x.size(3)\n",
    "        \n",
    "        # Divergencia KL con clipping para estabilidad\n",
    "        # Limitamos los valores extremos que podrían causar problemas numéricos\n",
    "        logvar = torch.clamp(logvar, min=-20, max=20)\n",
    "        mu = torch.clamp(mu, min=-20, max=20)\n",
    "        \n",
    "        # Fórmula KL estándar con mejor estabilidad numérica\n",
    "        kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        \n",
    "        # Verificamos que no haya NaNs en las pérdidas\n",
    "        if torch.isnan(recon_loss) or torch.isnan(kld_loss):\n",
    "            print(\"¡Alerta! NaN detectado en la función de pérdida\")\n",
    "            print(f\"recon_loss: {recon_loss.item() if not torch.isnan(recon_loss) else 'NaN'}\")\n",
    "            print(f\"kld_loss: {kld_loss.item() if not torch.isnan(kld_loss) else 'NaN'}\")\n",
    "            print(f\"mu min/max: {mu.min().item()}/{mu.max().item()}\")\n",
    "            print(f\"logvar min/max: {logvar.min().item()}/{logvar.max().item()}\")\n",
    "            # Proporcionar valores válidos si se detectan NaNs\n",
    "            if torch.isnan(recon_loss):\n",
    "                recon_loss = torch.tensor(1.0).to(recon_x.device)\n",
    "            if torch.isnan(kld_loss):\n",
    "                kld_loss = torch.tensor(1.0).to(recon_x.device)\n",
    "        \n",
    "        # Pérdida total - comenzamos con un peso KL muy bajo\n",
    "        return recon_loss + kld_weight * kld_loss, recon_loss, kld_loss\n",
    "    \n",
    "    def train(self, train_loader, val_loader, epochs=50, lr=1e-4, weight_decay=1e-5, kld_weight=0.0001):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Entrenamiento (train_loader ya solo contiene árboles sanos)\n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            train_recon_loss = 0\n",
    "            train_kld_loss = 0\n",
    "            \n",
    "            for data, _ in train_loader:\n",
    "                data = data.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                reconstructed, mu, logvar = self.model(data)\n",
    "                loss, recon_loss, kld_loss = self.vae_loss(reconstructed, data, mu, logvar, kld_weight)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                train_recon_loss += recon_loss.item()\n",
    "                train_kld_loss += kld_loss.item()\n",
    "            \n",
    "            # Normalizar pérdidas\n",
    "            train_loss /= len(train_loader.dataset)\n",
    "            train_recon_loss /= len(train_loader.dataset)\n",
    "            train_kld_loss /= len(train_loader.dataset)\n",
    "            train_losses.append(train_loss)\n",
    "            \n",
    "            # Validación (solo evaluamos el error de reconstrucción en árboles sanos)\n",
    "            self.model.eval()\n",
    "            val_loss = 0\n",
    "            val_recon_loss = 0\n",
    "            val_kld_loss = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for data, labels in val_loader:\n",
    "                    # Solo evaluar en árboles sanos\n",
    "                    healthy_mask = labels == 0\n",
    "                    if not any(healthy_mask):\n",
    "                        continue\n",
    "                        \n",
    "                    data = data[healthy_mask].to(self.device)\n",
    "                    reconstructed, mu, logvar = self.model(data)\n",
    "                    \n",
    "                    loss, recon_loss, kld_loss = self.vae_loss(reconstructed, data, mu, logvar, kld_weight)\n",
    "                    \n",
    "                    val_loss += loss.item()\n",
    "                    val_recon_loss += recon_loss.item()\n",
    "                    val_kld_loss += kld_loss.item()\n",
    "            \n",
    "            # Normalizar pérdidas\n",
    "            n_healthy = sum((labels.cpu().numpy() == 0).sum() for _, labels in val_loader)\n",
    "            if n_healthy > 0:\n",
    "                val_loss /= n_healthy\n",
    "                val_recon_loss /= n_healthy\n",
    "                val_kld_loss /= n_healthy\n",
    "                val_losses.append(val_loss)\n",
    "                \n",
    "                # Actualizar el scheduler\n",
    "                scheduler.step(val_loss)\n",
    "                \n",
    "                # Guardar el mejor modelo\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    torch.save(self.model.state_dict(), 'best_vae.pth')\n",
    "                    print(f'Modelo guardado época: {epoch+1}')\n",
    "            \n",
    "            print(f'Epoch {epoch+1}/{epochs}')\n",
    "            print(f'  Train Loss: {train_loss:.6f} (Recon: {train_recon_loss:.6f}, KLD: {train_kld_loss:.6f})')\n",
    "            print(f'  Val Loss: {val_loss:.6f} (Recon: {val_recon_loss:.6f}, KLD: {val_kld_loss:.6f})')\n",
    "        \n",
    "        # Cargar el mejor modelo\n",
    "        self.model.load_state_dict(torch.load('best_vae.pth'))\n",
    "        return train_losses, val_losses\n",
    "    \n",
    "    def calculate_threshold(self, val_loader, percentile=95):\n",
    "        \"\"\"Calcula el umbral basado en los datos de validación de árboles sanos\"\"\"\n",
    "        self.model.eval()\n",
    "        reconstruction_errors = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, labels in val_loader:\n",
    "                # Solo usar árboles sanos para calibrar el umbral\n",
    "                healthy_mask = labels == 0\n",
    "                if not any(healthy_mask):\n",
    "                    continue\n",
    "                    \n",
    "                healthy_data = data[healthy_mask].to(self.device)\n",
    "                reconstructed, _, _ = self.model(healthy_data)\n",
    "                \n",
    "                # Calcular error por imagen\n",
    "                for i in range(healthy_data.size(0)):\n",
    "                    error = torch.mean((reconstructed[i] - healthy_data[i])**2).item()\n",
    "                    reconstruction_errors.append(error)\n",
    "        \n",
    "        # Establecer umbral en el percentil especificado\n",
    "        self.threshold = np.percentile(reconstruction_errors, percentile)\n",
    "        print(f'Threshold set to: {self.threshold:.6f} (percentile {percentile})')\n",
    "        return self.threshold\n",
    "    \n",
    "    def predict(self, data_loader):\n",
    "        \"\"\"Predice si un árbol está enfermo basándose en el error de reconstrucción\"\"\"\n",
    "        if self.threshold is None:\n",
    "            raise ValueError(\"Threshold must be set before prediction using calculate_threshold\")\n",
    "        \n",
    "        self.model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        all_scores = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, labels in data_loader:\n",
    "                data = data.to(self.device)\n",
    "                reconstructed, _, _ = self.model(data)\n",
    "                \n",
    "                # Calcular error de reconstrucción por imagen\n",
    "                for i in range(data.size(0)):\n",
    "                    recon_error = torch.mean((reconstructed[i] - data[i])**2).item()\n",
    "                    # Si el error supera el umbral, clasificar como enfermo (anomalía)\n",
    "                    prediction = 1 if recon_error > self.threshold else 0\n",
    "                    all_preds.append(prediction)\n",
    "                    all_labels.append(labels[i].item())\n",
    "                    all_scores.append(recon_error)\n",
    "        \n",
    "        return np.array(all_preds), np.array(all_labels), np.array(all_scores)\n",
    "    \n",
    "    def evaluate(self, data_loader):\n",
    "        \"\"\"Evalúa el rendimiento del detector de anomalías\"\"\"\n",
    "        predictions, labels, scores = self.predict(data_loader)\n",
    "        \n",
    "        # Calcular métricas\n",
    "        accuracy = accuracy_score(labels, predictions)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "        \n",
    "        # Calcular AUC-ROC\n",
    "        auc_roc = roc_auc_score(labels, scores)\n",
    "        \n",
    "        results = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'auc_roc': auc_roc\n",
    "        }\n",
    "        \n",
    "        return results, predictions, labels, scores\n",
    "    \n",
    "    def generate_samples(self, num_samples=1):\n",
    "        \"\"\"Genera nuevas muestras desde el espacio latente\"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Muestrear del espacio latente (distribución normal)\n",
    "            z = torch.randn(num_samples, self.latent_dim).to(self.device)\n",
    "            # Decodificar\n",
    "            samples = self.model.decode(z)\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55efb576-165c-4495-b194-5336f107d733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training VAE...\n",
      "Modelo guardado época: 1\n",
      "Epoch 1/2\n",
      "  Train Loss: 5456831.797658 (Recon: 5456810.296956, KLD: 215277.801531)\n",
      "  Val Loss: 5158923.000000 (Recon: 5158922.003745, KLD: 9713.938817)\n",
      "Modelo guardado época: 2\n",
      "Epoch 2/2\n",
      "  Train Loss: 5039498.792506 (Recon: 5039497.792974, KLD: 9221.306856)\n",
      "  Val Loss: 4727160.554307 (Recon: 4727159.565543, KLD: 8574.221925)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10161/703926198.py:250: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load('best_vae.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating threshold...\n"
     ]
    }
   ],
   "source": [
    "beta=1.0\n",
    "latent_dim=128\n",
    "detector = VAEAnomalyDetector(\n",
    "        input_channels=input_channels, \n",
    "        device=device, \n",
    "        latent_dim=latent_dim,\n",
    "    )\n",
    "print(\"Training VAE...\")\n",
    "train_losses, val_losses = detector.train(\n",
    "        train_loader, \n",
    "        test_loader, \n",
    "        epochs=2, \n",
    "    )    # Calcular umbral\n",
    "print(\"Calculating threshold...\")\n",
    "threshold = detector.calculate_threshold(train_loader) \n",
    "    # Evaluar en el conjunto de prueba\n",
    "print(\"Evaluating model...\")\n",
    "results, predictions, labels, scores = detector.evaluate(test_loader)\n",
    "    # Imprimir resultados\n",
    "print(\"\\nResults:\")\n",
    "for metric, value in results.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69554d8c-4d85-432f-88b2-117ffc7fe930",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
