{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b5ba023-108d-4b74-8331-e78a7054004f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CherryTreeDataset import CherryTreeDataset\n",
    "from torchvision import transforms\n",
    "from auxii import plot_spectra\n",
    "from resnet_adapters import adapt_resnet_channels\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet18_Weights, ResNet50_Weights\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a6e1a2d-ff2c-4d06-b77e-d966c57de3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define las transformaciones si son necesarias\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    #transforms.Resize((1280, 960)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "#formats = ('RGB.JPG', 'RED.TIF','GRE.TIF','NIR.TIF','REG.TIF')\n",
    "#formats = ('RGB.JPG','NIR.TIF','REG.TIF')\n",
    "formats = ('RGB.JPG',)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbb8529a-eba4-47c5-82bc-921a0b5924ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "full_dataset = CherryTreeDataset('/mnt/d/Maestria/tfm/7144071', transform=transform, formats=formats, concatenate=True, balance=False)\n",
    "\n",
    "healthy_indices_full = [i for i, (_, label) in enumerate(full_dataset.samples) if label == 0]\n",
    "\n",
    "# División en conjuntos de entrenamiento y prueba\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# Ajustar los índices saludables al train_dataset\n",
    "healthy_indices_train = [i for i in range(len(train_dataset)) if i in train_dataset.indices and train_dataset.indices[i] in healthy_indices_full]\n",
    "\n",
    "# Configurar los DataLoader usando los índices filtrados\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, sampler=SubsetRandomSampler(healthy_indices_train), num_workers=20)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True, num_workers=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd6dc02b-e4eb-46cc-974a-32f5ac7526b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El dispositivo seleccionado es cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'El dispositivo seleccionado es {device}')\n",
    "\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder: Reducción progresiva de la dimensionalidad a través de más capas convolucionales\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, stride=2, padding=1),  # Reducción espacial (H/2, W/2)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),  # Reducción espacial (H/4, W/4)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.Conv2d(128, 264, 3, stride=2, padding=1),  # Reducción espacial (H/8, W/8)\n",
    "            nn.BatchNorm2d(264),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.Conv2d(264, 512, 3, stride=2, padding=1),  # Reducción espacial (H/16, W/16)\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.Conv2d(512, 1024, 3, stride=2, padding=1),  # Reducción espacial (H/32, W/32)\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        # Decoder: Recuperación progresiva de la dimensionalidad original\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 512, 3, stride=2, padding=1, output_padding=1),  # Expansión (H/16, W/16)\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(512, 264, 3, stride=2, padding=1, output_padding=1),  # Expansión (H/8, W/8)\n",
    "            nn.BatchNorm2d(264),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(264, 128, 3, stride=2, padding=1, output_padding=1),  # Expansión (H/4, W/4)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),  # Expansión (H/2, W/2)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(64, 3, 3, stride=2, padding=1, output_padding=1),  # Expansión a tamaño original (H, W)\n",
    "            nn.Tanh()  # Para asegurar que la salida esté en el rango [-1, 1]\n",
    "        )\n",
    "\n",
    "        # Inicialización de pesos\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Propagación hacia adelante del encoder y decoder\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        # Inicialización de los pesos de manera más robusta\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55efb576-165c-4495-b194-5336f107d733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250, Loss: 0.5872\n",
      "Epoch 2/250, Loss: 0.4581\n",
      "Epoch 3/250, Loss: 0.4311\n",
      "Epoch 4/250, Loss: 0.4098\n",
      "Epoch 5/250, Loss: 0.3901\n",
      "Epoch 6/250, Loss: 0.3734\n",
      "Epoch 7/250, Loss: 0.3646\n",
      "Epoch 8/250, Loss: 0.3557\n",
      "Epoch 9/250, Loss: 0.3439\n",
      "Epoch 10/250, Loss: 0.3349\n",
      "Epoch 11/250, Loss: 0.3300\n",
      "Epoch 12/250, Loss: 0.3255\n",
      "Epoch 13/250, Loss: 0.3194\n",
      "Epoch 14/250, Loss: 0.3146\n",
      "Epoch 15/250, Loss: 0.3101\n",
      "Epoch 16/250, Loss: 0.3061\n",
      "Epoch 17/250, Loss: 0.3026\n",
      "Epoch 18/250, Loss: 0.2991\n",
      "Epoch 19/250, Loss: 0.2970\n",
      "Epoch 20/250, Loss: 0.2940\n",
      "Epoch 21/250, Loss: 0.2921\n",
      "Epoch 22/250, Loss: 0.2897\n",
      "Epoch 23/250, Loss: 0.2884\n",
      "Epoch 24/250, Loss: 0.2870\n",
      "Epoch 25/250, Loss: 0.2851\n",
      "Epoch 26/250, Loss: 0.2836\n",
      "Epoch 27/250, Loss: 0.2822\n",
      "Epoch 28/250, Loss: 0.2805\n",
      "Epoch 29/250, Loss: 0.2797\n",
      "Epoch 30/250, Loss: 0.2797\n",
      "Epoch 31/250, Loss: 0.2773\n",
      "Epoch 32/250, Loss: 0.2768\n",
      "Epoch 33/250, Loss: 0.2758\n",
      "Epoch 34/250, Loss: 0.2752\n",
      "Epoch 35/250, Loss: 0.2738\n",
      "Epoch 36/250, Loss: 0.2730\n",
      "Epoch 37/250, Loss: 0.2722\n",
      "Epoch 38/250, Loss: 0.2724\n",
      "Epoch 39/250, Loss: 0.2713\n",
      "Epoch 40/250, Loss: 0.2701\n",
      "Epoch 41/250, Loss: 0.2693\n",
      "Epoch 42/250, Loss: 0.2689\n",
      "Epoch 43/250, Loss: 0.2688\n",
      "Epoch 44/250, Loss: 0.2680\n",
      "Epoch 45/250, Loss: 0.2672\n",
      "Epoch 46/250, Loss: 0.2665\n",
      "Epoch 47/250, Loss: 0.2659\n",
      "Epoch 48/250, Loss: 0.2653\n",
      "Epoch 49/250, Loss: 0.2649\n",
      "Epoch 50/250, Loss: 0.2650\n",
      "Epoch 51/250, Loss: 0.2643\n",
      "Epoch 52/250, Loss: 0.2641\n",
      "Epoch 53/250, Loss: 0.2627\n",
      "Epoch 54/250, Loss: 0.2626\n",
      "Epoch 55/250, Loss: 0.2621\n",
      "Epoch 56/250, Loss: 0.2631\n",
      "Epoch 57/250, Loss: 0.2616\n",
      "Epoch 58/250, Loss: 0.2618\n",
      "Epoch 59/250, Loss: 0.2609\n",
      "Epoch 60/250, Loss: 0.2605\n",
      "Epoch 61/250, Loss: 0.2600\n",
      "Epoch 62/250, Loss: 0.2596\n",
      "Epoch 63/250, Loss: 0.2593\n",
      "Epoch 64/250, Loss: 0.2592\n",
      "Epoch 65/250, Loss: 0.2587\n",
      "Epoch 66/250, Loss: 0.2583\n",
      "Epoch 67/250, Loss: 0.2583\n",
      "Epoch 68/250, Loss: 0.2576\n",
      "Epoch 69/250, Loss: 0.2579\n",
      "Epoch 70/250, Loss: 0.2573\n",
      "Epoch 71/250, Loss: 0.2571\n",
      "Epoch 72/250, Loss: 0.2570\n",
      "Epoch 73/250, Loss: 0.2566\n",
      "Epoch 74/250, Loss: 0.2558\n",
      "Epoch 75/250, Loss: 0.2566\n",
      "Epoch 76/250, Loss: 0.2550\n",
      "Epoch 77/250, Loss: 0.2552\n",
      "Epoch 78/250, Loss: 0.2550\n",
      "Epoch 79/250, Loss: 0.2552\n",
      "Epoch 80/250, Loss: 0.2544\n",
      "Epoch 81/250, Loss: 0.2537\n",
      "Epoch 82/250, Loss: 0.2537\n",
      "Epoch 83/250, Loss: 0.2532\n",
      "Epoch 84/250, Loss: 0.2533\n",
      "Epoch 85/250, Loss: 0.2533\n",
      "Epoch 86/250, Loss: 0.2533\n",
      "Epoch 87/250, Loss: 0.2531\n",
      "Epoch 88/250, Loss: 0.2525\n",
      "Epoch 89/250, Loss: 0.2522\n"
     ]
    }
   ],
   "source": [
    "model = ConvAutoencoder()\n",
    "model.to(device)  # Mueve el modelo al dispositivo apropiado (CPU o GPU)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)  # Define el optimizador\n",
    "criterion = nn.MSELoss()  # Define la función de pérdida\n",
    "\n",
    "# DataLoader para los datos de entrenamiento\n",
    "# Asegúrate de que el DataLoader solo devuelva datos normales para el entrenamiento\n",
    "\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs=2):\n",
    "    model.train()  # Poner el modelo en modo de entrenamiento\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, _ in dataloader:  # Ignora las etiquetas porque no son necesarias para el entrenamiento\n",
    "            inputs = inputs.to(device)  # Mueve los datos al dispositivo\n",
    "\n",
    "            optimizer.zero_grad()  # Limpia los gradientes de los parámetros del optimizador\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs)  # Calcula la pérdida de reconstrucción\n",
    "\n",
    "            # Backward pass y optimización\n",
    "            loss.backward()  # Calcula los gradientes\n",
    "            optimizer.step()  # Actualiza los pesos\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)  # Acumula la pérdida del lote\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloader.dataset)  # Calcula la pérdida promedio por época\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
    "\n",
    "# Llamar a la función de entrenamiento\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b1fa80-ca48-418e-9c06-8534f6cdbe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "def calculate_reconstruction_error(model, dataloader, device):\n",
    "    model.eval()\n",
    "    reconstruction_errors = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data, labels in dataloader:\n",
    "            data = data.to(device)\n",
    "            reconstructions = model(data)\n",
    "            mse = torch.nn.functional.mse_loss(reconstructions, data, reduction='none')\n",
    "            mse = mse.mean([1, 2, 3])  # Promedio por imagen\n",
    "            reconstruction_errors.extend(mse.cpu().numpy())\n",
    "            true_labels.extend(labels.numpy())  # Asumiendo que las etiquetas están en un tensor\n",
    "    return np.array(reconstruction_errors), np.array(true_labels)\n",
    "\n",
    "def determine_threshold(errors, percentile=95):\n",
    "    return np.percentile(errors, percentile)\n",
    "\n",
    "def detect_anomalies(errors, threshold):\n",
    "    return errors > threshold\n",
    "\n",
    "# Calcula errores de reconstrucción\n",
    "errors, true_labels = calculate_reconstruction_error(model, test_loader, device)\n",
    "threshold = determine_threshold(errors)\n",
    "predicted_labels = detect_anomalies(errors, threshold).astype(int)\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.xticks(np.arange(len(classes)), classes, rotation=45)\n",
    "    plt.yticks(np.arange(len(classes)), classes, rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "# Supongamos que las clases son [0, 1] donde '0' es 'Healthy' y '1' es 'Disease'\n",
    "plot_confusion_matrix(cm, classes=['Healthy', 'Disease'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944dfd41-635e-4f41-ae18-5146cd0530dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, label = full_dataset[25]\n",
    "\n",
    "# Convertir la imagen al dispositivo adecuado y añadir una dimensión de lote si es necesario\n",
    "image = images.to(device).unsqueeze(0)\n",
    "# Asegurarse de que el modelo está en modo evaluación\n",
    "model.eval()\n",
    "\n",
    "# Obtener la reconstrucción\n",
    "with torch.no_grad():\n",
    "    reconstruction = model(image)\n",
    "\n",
    "# Quitar la dimensión del lote para visualización y mover a CPU\n",
    "input_image = image.squeeze(0).cpu()\n",
    "reconstructed_image = reconstruction.squeeze(0).cpu()\n",
    "\n",
    "# Usar la función plot_spectra para visualizar las imágenes\n",
    "plot_spectra(input_image, f'Original - {label}')\n",
    "plot_spectra(reconstructed_image, f'R - {label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ff3eab-1ac9-4e10-8877-dab6dd3daad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f633635f-97df-446c-b91f-9de41b0114cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273721bc-e2fc-4ff2-a8ee-024f124a71ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
